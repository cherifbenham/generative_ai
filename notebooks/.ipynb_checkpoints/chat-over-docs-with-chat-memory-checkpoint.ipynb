{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dda6e5d",
   "metadata": {},
   "source": [
    "The difference between this chain and a QA and a retrieval QA chain is that this allows for passing in a chat history which can be used to allow for follow up questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e926865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014825b",
   "metadata": {},
   "source": [
    "# loading in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1276d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bddad9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1=\"../input/state_of_the_union.txt\" ## 100 pages\n",
    "file_2=\"../input/CELEX_32022L2464_EN_TXT.pdf\" ## 300 pages\n",
    "file_3=\"../input/input_gri.pdf\" ## 900 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8cc074ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 ms, sys: 1.02 ms, total: 2.58 ms\n",
      "Wall time: 1.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader_1=TextLoader(file_1)\n",
    "loader_2=PyPDFLoader(file_2)\n",
    "loader_3=PyPDFLoader(file_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ca04ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 513 ms, total: 27.8 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loaders=[loader_1, loader_2, loader_3]\n",
    "docs=[]\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfa7b7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017', metadata={'source': '../input/1706.03762.pdf', 'page': 0}),\n",
       " Document(page_content='transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht\\x001and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence\\nof continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\\nsequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive', metadata={'source': '../input/1706.03762.pdf', 'page': 1}),\n",
       " Document(page_content='of continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\\nsequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n2', metadata={'source': '../input/1706.03762.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 1: The Transformer - model architecture.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3', metadata={'source': '../input/1706.03762.pdf', 'page': 2}),\n",
       " Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': '../input/1706.03762.pdf', 'page': 3}),\n",
       " Document(page_content='Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\n5', metadata={'source': '../input/1706.03762.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2\\x01d) O(1) O(1)\\nRecurrent O(n\\x01d2) O(n) O(n)\\nConvolutional O(k\\x01n\\x01d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\\x01n\\x01d)O(1) O(n=r)\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos;2i)=sin(pos=100002i=d model)\\nPE(pos;2i+1)=cos(pos=100002i=d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\x19to10000\\x012\\x19. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', metadata={'source': '../input/1706.03762.pdf', 'page': 5}),\n",
       " Document(page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n=r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\\x01n\\x01d+n\\x01d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with\\x0c1= 0:9,\\x0c2= 0:98and\\x0f= 10\\x009. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d\\x000:5\\nmodel\\x01min(step_num\\x000:5;step _num\\x01warmup _steps\\x001:5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0:1.\\n7', metadata={'source': '../input/1706.03762.pdf', 'page': 6}),\n",
       " Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1:0\\x011020\\nGNMT + RL [38] 24.6 39.92 2:3\\x0110191:4\\x011020\\nConvS2S [9] 25.16 40.46 9:6\\x0110181:5\\x011020\\nMoE [32] 26.03 40.56 2:0\\x0110191:2\\x011020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8:0\\x011020\\nGNMT + RL Ensemble [38] 26.30 41.16 1:8\\x0110201:1\\x011021\\nConvS2S Ensemble [9] 26.36 41.29 7:7\\x0110191:2\\x011021\\nTransformer (base model) 27.3 38.1 3:3\\x011018\\nTransformer (big) 28.4 41.8 2:3\\x011019\\nLabel Smoothing During training, we employed label smoothing of value \\x0fls= 0:1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0\\nBLEU, establishing a new state-of-the-art BLEU score of 28:4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,\\noutperforming all of the previously published single models, at less than 1=4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0:1, instead of 0:3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty \\x0b= 0:6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': '../input/1706.03762.pdf', 'page': 7}),\n",
       " Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdrop\\x0flstrain PPL BLEU params\\nsteps (dev) (dev)\\x02106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural\\n9', metadata={'source': '../input/1706.03762.pdf', 'page': 8}),\n",
       " Document(page_content='constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\nincreased the maximum output length to input length + 300. We used a beam size of 21and\\x0b= 0:3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n10', metadata={'source': '../input/1706.03762.pdf', 'page': 9}),\n",
       " Document(page_content='[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n11', metadata={'source': '../input/1706.03762.pdf', 'page': 10}),\n",
       " Document(page_content='[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': '../input/1706.03762.pdf', 'page': 11}),\n",
       " Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': '../input/1706.03762.pdf', 'page': 12}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': '../input/1706.03762.pdf', 'page': 13}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': '../input/1706.03762.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_4=\"../input/1706.03762.pdf\"\n",
    "loader_4=PyPDFLoader(file_4)\n",
    "\n",
    "\n",
    "loader_4.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa13a0c",
   "metadata": {},
   "source": [
    "# Chunking the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6fbf1b",
   "metadata": {},
   "source": [
    "Now that the documents were created (using the loaders), we need to split them and create a new object 'documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "48a2a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 ms, sys: 165 µs, total: 1.64 ms\n",
      "Wall time: 1.66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_splitter=CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c55d5862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017', metadata={'source': '../input/1706.03762.pdf', 'page': 0}),\n",
       " Document(page_content='transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht\\x001and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence\\nof continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\\nsequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n2', metadata={'source': '../input/1706.03762.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 1: The Transformer - model architecture.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3', metadata={'source': '../input/1706.03762.pdf', 'page': 2}),\n",
       " Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': '../input/1706.03762.pdf', 'page': 3}),\n",
       " Document(page_content='Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\n5', metadata={'source': '../input/1706.03762.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2\\x01d) O(1) O(1)\\nRecurrent O(n\\x01d2) O(n) O(n)\\nConvolutional O(k\\x01n\\x01d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\\x01n\\x01d)O(1) O(n=r)\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos;2i)=sin(pos=100002i=d model)\\nPE(pos;2i+1)=cos(pos=100002i=d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\x19to10000\\x012\\x19. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', metadata={'source': '../input/1706.03762.pdf', 'page': 5}),\n",
       " Document(page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n=r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\\x01n\\x01d+n\\x01d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with\\x0c1= 0:9,\\x0c2= 0:98and\\x0f= 10\\x009. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d\\x000:5\\nmodel\\x01min(step_num\\x000:5;step _num\\x01warmup _steps\\x001:5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0:1.\\n7', metadata={'source': '../input/1706.03762.pdf', 'page': 6}),\n",
       " Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1:0\\x011020\\nGNMT + RL [38] 24.6 39.92 2:3\\x0110191:4\\x011020\\nConvS2S [9] 25.16 40.46 9:6\\x0110181:5\\x011020\\nMoE [32] 26.03 40.56 2:0\\x0110191:2\\x011020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8:0\\x011020\\nGNMT + RL Ensemble [38] 26.30 41.16 1:8\\x0110201:1\\x011021\\nConvS2S Ensemble [9] 26.36 41.29 7:7\\x0110191:2\\x011021\\nTransformer (base model) 27.3 38.1 3:3\\x011018\\nTransformer (big) 28.4 41.8 2:3\\x011019\\nLabel Smoothing During training, we employed label smoothing of value \\x0fls= 0:1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0\\nBLEU, establishing a new state-of-the-art BLEU score of 28:4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,\\noutperforming all of the previously published single models, at less than 1=4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0:1, instead of 0:3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty \\x0b= 0:6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': '../input/1706.03762.pdf', 'page': 7}),\n",
       " Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdrop\\x0flstrain PPL BLEU params\\nsteps (dev) (dev)\\x02106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural\\n9', metadata={'source': '../input/1706.03762.pdf', 'page': 8}),\n",
       " Document(page_content='constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\nincreased the maximum output length to input length + 300. We used a beam size of 21and\\x0b= 0:3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n10', metadata={'source': '../input/1706.03762.pdf', 'page': 9}),\n",
       " Document(page_content='[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n11', metadata={'source': '../input/1706.03762.pdf', 'page': 10}),\n",
       " Document(page_content='[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': '../input/1706.03762.pdf', 'page': 11}),\n",
       " Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': '../input/1706.03762.pdf', 'page': 12}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': '../input/1706.03762.pdf', 'page': 13}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': '../input/1706.03762.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader_4.load_and_split(CharacterTextSplitter(chunk_size=100, chunk_overlap=0))\n",
    "for doc in docs:\n",
    "    print(doc.metadata[\"page\"])\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727aebd",
   "metadata": {},
   "source": [
    "# Putting the docs in a vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed756ff5",
   "metadata": {},
   "source": [
    "Which will allow to do semantic search over them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe426fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.34 s, sys: 214 ms, total: 2.56 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore=Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec095310",
   "metadata": {},
   "source": [
    "# Create the memory object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bf463",
   "metadata": {},
   "source": [
    "Which is necessary to track the inputs/outputs and hold a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e68aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82279e63",
   "metadata": {},
   "source": [
    "# Initialise the conversation retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ab0873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(OpenAI(temperature=0),vectorstore.as_retriever(),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01b8e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what did biden say about Ketanji Brown Jackson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981833ea",
   "metadata": {},
   "source": [
    "# using different language models to condense and answer the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adbba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(model='gpt-4', temperature=0),\n",
    "    vectorstore.as_retriever(),\n",
    "    condense_question_llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c04b5",
   "metadata": {},
   "source": [
    "# return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "267b1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    vectorstore.as_retriever(),\n",
    "    return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "58291809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-davinci-003'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=OpenAI(temperature=0)\n",
    "test.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f62e1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "----------- answer ----------------\n",
      "-----------------------------------\n",
      " Justice Stephen Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.\n",
      "-----------------------------------\n",
      "----------- page content ----------\n",
      "-----------------------------------\n",
      "In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \n",
      "\n",
      "We cannot let this happen. \n",
      "\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \n",
      "\n",
      "A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n",
      "\n",
      "And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n",
      "\n",
      "We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n",
      "\n",
      "We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n",
      "\n",
      "We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n",
      "\n",
      "We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \n",
      "\n",
      "We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours.\n",
      "-----------------------------------\n",
      "----------- source ----------------\n",
      "-----------------------------------\n",
      "../input/state_of_the_union.txt\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"who is Justice Stephen Breyer—an\"\n",
    "result = qa({'question':query,'chat_history':chat_history})\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('----------- answer ----------------')\n",
    "print('-----------------------------------')\n",
    "print(result['answer'])\n",
    "print('-----------------------------------')\n",
    "print('----------- page content ----------')\n",
    "print('-----------------------------------')\n",
    "print(result['source_documents'][0].page_content)\n",
    "print('-----------------------------------')\n",
    "print('----------- source ----------------')\n",
    "print('-----------------------------------')\n",
    "print(result['source_documents'][0].metadata['source'])\n",
    "print('-----------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34c914",
   "metadata": {},
   "source": [
    "# define a chat history function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b5df706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_history(inputs)->str:\n",
    "    res=[]\n",
    "    for human,ai in inputs:\n",
    "        res.append(f'Human:{human}\\nAI:{ai}')\n",
    "    return '\\n'.join(res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8e407a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:a\\nAI:b\\nHuman:c\\nAI:d'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chat_history([(\"a\",\"b\"),(\"c\",\"d\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92403f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    vectorstore.as_retriever(),\n",
    "    get_chat_history=get_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01339a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what the work-related risks for employees',\n",
       " 'chat_history': [],\n",
       " 'answer': ' Work-related risks for employees can include exposure to hazardous materials, high-consequence work-related injuries, and high-potential work-related incidents. Mental illness can also be a work-related risk if it is notified voluntarily by the worker and is supported by an opinion from a licensed healthcare professional with appropriate training and experience. Organizations should use the hierarchy of controls to eliminate, substitute, control, and minimize the risks.',\n",
       " 'source_documents': [Document(page_content='Note 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.\\nBibliography\\nGRI 403: Occupational Health and Safety 2018\\n686', metadata={'source': '../input/input_gri.pdf', 'page': 685}),\n",
       "  Document(page_content='employee\\nindividual who is in an employment relationship with the organization according to national law\\nor practice\\nexposure\\nquantity of time spent at or the nature of contact with certain environments that possess various\\ndegrees and kinds of hazard , or proximity to a condition that might cause injury  or ill health  (e.g.,\\nchemicals, radiation, high pressure, noise, fire, explosives)\\nformal agreement\\nwritten document signed by all relevant parties declaring a mutual intention to abide by what is\\nstipulated in the document\\nExamples: a local collective  bargaining  agreement, a national or international framework\\nagreement\\nformal joint management–worker health and safety committee\\ncommittee composed of management and worker  representatives , whose function is integrated\\ninto an organizational structure, and which operates according to agreed written policies,\\nprocedures, and rules, and helps facilitate worker participation  and consultation  on matters of\\noccupational health and safety\\nfull-time employee\\nemployee  whose working hours per week, month, or year are defined according to national law\\nor practice regarding working time\\nhealth promotion\\nprocess of enabling people to increase control over and improve their health\\nSource: World Health Organization (WHO), Ottawa Charter for Health Promotion , 1986\\nNote: The terms ‘health promotion’, ‘wellbeing’, and ‘wellness’ are often used\\ninterchangeably.\\nhierarchy of controls\\nsystematic approach to enhance occupational health and safety, eliminate hazards , and\\nminimize risks\\nNote 1: The hierarchy of controls seeks to protect workers by ranking the ways in which\\nhazards can be controlled. Each control in the hierarchy is considered less effective\\nthan the one before it. The priority is to eliminate the hazard, which is the most\\neffective way to control it.\\nNote 2: The International Labour Organization (ILO) Guidelines on Occupational Safety and\\nHealth Management Systems , 2001 and ISO 45001:2018 list the following\\npreventive and protective measures in the following order of priority:\\nhigh-consequence work-related injury\\nwork-related  injury  that results in a fatality or in an injury from which the worker cannot, does not,\\nor is not expected to recover fully to pre-injury health status within six months\\nhigh-potential work-related incident\\nwork-related  incident  with a high probability of causing a high-consequence  injuryE\\nF\\nH\\neliminate the hazard/risk;•\\nsubstitute the hazard/risk with less hazardous processes, operations,\\nmaterials, or equipment;•\\ncontrol the hazard/risk at source, through the use of engineering controls or\\norganizational measures;•\\nminimize the hazard/risk by the design of safe work systems, which include\\nadministrative control measures;•\\nwhere residual hazards/risks cannot be controlled by collective measures,\\nprovide for appropriate personal protective equipment, including clothing, at no\\ncost, and implement measures to ensure its use and maintenance.•\\nGRI 403: Occupational Health and Safety 2018\\n681', metadata={'source': '../input/input_gri.pdf', 'page': 680}),\n",
       "  Document(page_content='Note 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nBibliography\\nGRI 12: Coal Sector 2022\\n287', metadata={'source': '../input/input_gri.pdf', 'page': 286}),\n",
       "  Document(page_content='In cases where an organization has no control over both the work and workplace, it still has a responsibility to make\\nefforts, including exercising any leverage it might have, to prevent and mitigate negative occupational health and safety\\nimpacts that are directly linked to its operations, products, or services by its business relationships.\\nIn these cases, the organization is required, at a minimum, to describe its approach to preventing and mitigating\\nsignificant negative occupational health and safety impacts and the related hazards and risks, using Disclosure  403-\\n7 in the Topic management disclosures section.\\n \\nTable 1\\nExamples of employees and workers who are not employees according to the criteria of ‘control of work’ and\\n‘control of workplace’\\n Control of work\\nThe organization has sole control of the work, or\\nshares control with one or more organizationsNo control of work\\nThe organization has no control of the work\\nControl of\\nworkplace\\nThe\\norganization\\nhas sole\\ncontrol of the\\nworkplace, or\\nshares\\ncontrol with\\none or more\\norganizationsExamples:\\nEmployees of the reporting organization working\\nat a workplace controlled by the organization.\\nContractor hired by the reporting organization to\\nperform work that would otherwise be carried out\\nby an employee, at a workplace controlled by the\\norganization.\\nVolunteers performing work for the reporting\\norganization, at a workplace controlled by the\\norganization.Example:\\nWorkers of an equipment supplier to the\\nreporting organization who, at a workplace\\ncontrolled by the organization, perform regular\\nmaintenance on the supplier’s equipment (e.g.,\\nphotocopier) as stipulated in the contract\\nbetween the equipment supplier and the\\norganization. In this case, the organization has\\ncontrol over the workplace but not over the work\\ndone by the equipment supplier’s workers in its\\nworkplace.\\nNo control of\\nworkplace\\nThe\\norganization\\nhas no\\ncontrol of the\\nworkplaceExamples:\\nEmployees of the reporting organization working\\nat sites other than those controlled by the\\norganization (e.g., at home or in a public area, on\\ndomestic or international temporary work\\nassignments, or on business travels organized\\nby the organization).\\nContractors hired by the reporting organization to\\nperform work in a public area (e.g., on a road, on\\nthe street).\\nContractors hired by the reporting organization to\\ndeliver the work/service directly at the workplace\\nof a client of the organization.\\nWorkers of a supplier to the reporting\\norganization who work on the supplier’s\\npremises, and where the organization instructs\\nthe supplier to use particular materials or work\\nmethods in manufacturing/delivering the\\nrequired goods or services.Example:\\nWorkers of a supplier contracted by the reporting\\norganization who work on the supplier’s\\npremises using the supplier’s work methods.\\nFor instance, the reporting organization sources\\nbuttons and thread from a supplier, which are\\nstandard products of the supplier. The supplier’s\\nworkers make the buttons and thread at the\\nsupplier’s workplace. The organization, however,\\nlearns that the buttons are coated with a sealant\\nthat releases toxic gases when being applied by\\nworkers, thereby affecting their health. In this\\ncase, the organization has no control over both\\nthe work and workplace of the supplier’s\\nworkers, but its products are directly linked to\\nsignificant occupational health and safety\\nimpacts on those workers by its business\\nrelationship with the supplier.\\nGRI 403: Occupational Health and Safety 2018\\n661', metadata={'source': '../input/input_gri.pdf', 'page': 660})]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query = \"what the work-related risks for employees\"\n",
    "result=qa({'question':query,'chat_history':chat_history})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "761cd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((result['question'],result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "882c186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what the work-related risks for employees',\n",
       "  ' Work-related risks for employees can include exposure to hazardous materials, high-consequence work-related injuries, and high-potential work-related incidents. Mental illness can also be a work-related risk if it is notified voluntarily by the worker and is supported by an opinion from a licensed healthcare professional with appropriate training and experience. Organizations should use the hierarchy of controls to eliminate, substitute, control, and minimize the risks.')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0ec6668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:what the work-related risks for employees\\nAI: Work-related risks for employees can include exposure to hazardous materials, high-consequence work-related injuries, and high-potential work-related incidents. Mental illness can also be a work-related risk if it is notified voluntarily by the worker and is supported by an opinion from a licensed healthcare professional with appropriate training and experience. Organizations should use the hierarchy of controls to eliminate, substitute, control, and minimize the risks.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chat_history(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd0830ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'cite me other risks that were not cited in your last answer',\n",
       " 'chat_history': [('what the work-related risks for employees',\n",
       "   ' Work-related risks for employees can include exposure to hazardous materials, high-consequence work-related injuries, and high-potential work-related incidents. Mental illness can also be a work-related risk if it is notified voluntarily by the worker and is supported by an opinion from a licensed healthcare professional with appropriate training and experience. Organizations should use the hierarchy of controls to eliminate, substitute, control, and minimize the risks.')],\n",
       " 'answer': ' Other work-related risks for employees include exposure to hazardous environments, such as chemicals, radiation, high pressure, noise, fire, and explosives. Additionally, employees may be at risk of high-consequence work-related injuries or high-potential work-related incidents.',\n",
       " 'source_documents': [Document(page_content='Note 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.\\nBibliography\\nGRI 403: Occupational Health and Safety 2018\\n686', metadata={'source': '../input/input_gri.pdf', 'page': 685}),\n",
       "  Document(page_content='work-related injury or ill health\\nnegative impacts on health arising from exposure  to hazards  at work\\nSource: International Labour Organization (ILO), Guidelines on Occupational Safety and\\nHealth Management Systems, ILO-OSH 2001 , 2001; modified\\nNote 1: ‘Ill health’ indicates damage to health and includes diseases, illnesses, and\\ndisorders. The terms ‘disease’, ‘illness’, and ‘disorder’ are often used\\ninterchangeably and refer to conditions with specific symptoms and diagnoses. \\nNote 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nGRI Standards Glossary 2022\\n869', metadata={'source': '../input/input_gri.pdf', 'page': 868}),\n",
       "  Document(page_content='Note 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nBibliography\\nGRI 12: Coal Sector 2022\\n287', metadata={'source': '../input/input_gri.pdf', 'page': 286}),\n",
       "  Document(page_content='employee\\nindividual who is in an employment relationship with the organization according to national law\\nor practice\\nexposure\\nquantity of time spent at or the nature of contact with certain environments that possess various\\ndegrees and kinds of hazard , or proximity to a condition that might cause injury  or ill health  (e.g.,\\nchemicals, radiation, high pressure, noise, fire, explosives)\\nformal agreement\\nwritten document signed by all relevant parties declaring a mutual intention to abide by what is\\nstipulated in the document\\nExamples: a local collective  bargaining  agreement, a national or international framework\\nagreement\\nformal joint management–worker health and safety committee\\ncommittee composed of management and worker  representatives , whose function is integrated\\ninto an organizational structure, and which operates according to agreed written policies,\\nprocedures, and rules, and helps facilitate worker participation  and consultation  on matters of\\noccupational health and safety\\nfull-time employee\\nemployee  whose working hours per week, month, or year are defined according to national law\\nor practice regarding working time\\nhealth promotion\\nprocess of enabling people to increase control over and improve their health\\nSource: World Health Organization (WHO), Ottawa Charter for Health Promotion , 1986\\nNote: The terms ‘health promotion’, ‘wellbeing’, and ‘wellness’ are often used\\ninterchangeably.\\nhierarchy of controls\\nsystematic approach to enhance occupational health and safety, eliminate hazards , and\\nminimize risks\\nNote 1: The hierarchy of controls seeks to protect workers by ranking the ways in which\\nhazards can be controlled. Each control in the hierarchy is considered less effective\\nthan the one before it. The priority is to eliminate the hazard, which is the most\\neffective way to control it.\\nNote 2: The International Labour Organization (ILO) Guidelines on Occupational Safety and\\nHealth Management Systems , 2001 and ISO 45001:2018 list the following\\npreventive and protective measures in the following order of priority:\\nhigh-consequence work-related injury\\nwork-related  injury  that results in a fatality or in an injury from which the worker cannot, does not,\\nor is not expected to recover fully to pre-injury health status within six months\\nhigh-potential work-related incident\\nwork-related  incident  with a high probability of causing a high-consequence  injuryE\\nF\\nH\\neliminate the hazard/risk;•\\nsubstitute the hazard/risk with less hazardous processes, operations,\\nmaterials, or equipment;•\\ncontrol the hazard/risk at source, through the use of engineering controls or\\norganizational measures;•\\nminimize the hazard/risk by the design of safe work systems, which include\\nadministrative control measures;•\\nwhere residual hazards/risks cannot be controlled by collective measures,\\nprovide for appropriate personal protective equipment, including clothing, at no\\ncost, and implement measures to ensure its use and maintenance.•\\nGRI 403: Occupational Health and Safety 2018\\n681', metadata={'source': '../input/input_gri.pdf', 'page': 680})]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"cite me other risks that were not cited in your last answer\"\n",
    "result=qa({'question':query,'chat_history':chat_history})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24f2bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "\n",
    "query=\"please define high-potential work-related incident\"\n",
    "qa=ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    vectorstore.as_retriever(),\n",
    "    return_source_documents=True)\n",
    "\n",
    "result=qa({'question':query,'chat_history':chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6795b1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'please define high-potential work-related incident',\n",
       " 'chat_history': [],\n",
       " 'answer': ' A high-potential work-related incident is an occurrence arising out of or in the course of work that has the potential to result in injury or ill health. Examples of high-potential incidents include electrical problems, explosions, fires, overflows, overturning, leakage, flow, breakage, bursting, splitting, loss of control, slipping, stumbling and falling, body movement without stress, body movement under/with stress, shock, fright, and workplace violence or harassment.',\n",
       " 'source_documents': [Document(page_content=\"Note: Hazards can be:\\nwork-related incident\\noccurrence arising out of or in the course of work that could or does result in injury  or ill health\\nSource: International Organization for Standardization. ISO 45001:2018. Occupational health\\nand safety management systems — Requirements with guidance for use. Geneva:\\nISO, 2018; modified\\nDefinitions that are based on or come from the ISO 14046:2014 and ISO 45001:2018 standards\\nare reproduced with the permission of the International Organization for Standardization, ISO.\\nCopyright remains with ISO.\\nNote 1: Incidents might be due to, for example, electrical problems, explosion, fire; overflow,\\noverturning, leakage, flow; breakage, bursting, splitting; loss of control, slipping,\\nstumbling and falling; body movement without stress; body movement under/with\\nstress; shock, fright; workplace violence or harassment (e.g., sexual harassment).\\nNote 2: An incident that results in injury or ill health is often referred to as an ‘accident’. An\\nincident that has the potential to result in injury or ill health but where none occurs is\\noften referred to as a ‘ close  call', ‘near-miss’, or ‘near-hit’.\\nwork-related injury or ill health\\nnegative impacts on health arising from exposure  to hazards  at work\\nSource: International Labour Organization (ILO), Guidelines on Occupational Safety and\\nHealth Management Systems, ILO-OSH 2001 , 2001; modified\\nNote 1: ‘Ill health’ indicates damage to health and includes diseases, illnesses, and\\ndisorders. The terms ‘disease’, ‘illness’, and ‘disorder’ are often used\\ninterchangeably and refer to conditions with specific symptoms and diagnoses. \\nNote 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:physical (e.g., radiation, temperature extremes, constant loud noise, spills on\\nfloors or tripping hazards, unguarded machinery, faulty electrical equipment);•\\nergonomic (e.g., improperly adjusted workstations and chairs, awkward\\nmovements, vibration);•\\nchemical (e.g., exposure to solvents, carbon monoxide, flammable materials, or\\npesticides);•\\nbiological (e.g., exposure to blood and bodily fluids, fungi, bacteria, viruses, or\\ninsect bites);•\\npsychosocial (e.g., verbal abuse, harassment, bullying);•\\nrelated to work-organization (e.g., excessive workload demands, shift work, long\\nhours, night work, workplace violence).•\\na worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nGRI 403: Occupational Health and Safety 2018\\n685\", metadata={'source': '../input/input_gri.pdf', 'page': 684}),\n",
       "  Document(page_content='The definition of ‘high-consequence work-related injury’ uses ‘recovery time’, instead of ‘lost\\ntime’, as the criterion for determining the severity of an injury. Lost time is an indicator of the loss\\nof productivity for an organization as a result of a work-related injury; it does not necessarily\\nindicate the extent of harm suffered by a worker.\\n‘Recovery time’, in contrast, refers to the time needed for a worker to recover fully to pre-injury\\nhealth status; it does not refer to the time needed for a worker to return to work. In some cases,\\na worker might return to work before full recovery.\\nIn addition to reporting information on high-consequence work-related injuries based on\\nrecovery time as required by this disclosure, the organization can also report the number and\\nrate of work-related injuries that resulted in lost-workday cases, the average number of lost days\\nper lost-workday case, the number of lost workdays, and the absentee rate.\\nGuidance  for Disclosure 403-9-c\\nThis disclosure covers work-related hazards  that pose a risk of high-consequence  injury  if not\\ncontrolled, even when there are control measures in place. The hazards might have been\\nidentified proactively through risk assessment, or reactively as a result of either a high-potential\\nincident  or a high-consequence injury.\\nExamples of work-related hazards causing or contributing to high-consequence injuries include\\nexcessive workload demands, tripping hazards, or exposure to flammable materials.\\nIf the identified work-related hazards vary significantly across different locations, the organization\\nmay group or disaggregate these by relevant categories, such as by geographical area or\\nbusiness line. Similarly, if there are a high number of hazards, the organization may group or\\ncategorize them to facilitate reporting.\\nWhen reporting how it has determined which work-related hazards pose a risk of high-\\nconsequence injury using Disclosure 403-9-c-i, the organization can describe the criteria or\\nthreshold used to determine which hazards pose such a risk and which do not. The processes\\nto identify hazards and assess risks, and to apply the hierarchy of controls, are reported using\\nDisclosure  403-2-a .\\nDisclosure 403-9-c-ii does not require reporting which work-related hazards have caused or\\ncontributed to which high-consequence injuries during the reporting period; it requires the\\naggregate analysis of all work-related hazards that resulted in high-consequence injuries.\\nIf a work-related incident resulting in a high-consequence injury is still under investigation at the\\nend of the reporting period, the organization can state this in its reported information. The\\norganization can report on actions taken during the reporting period to eliminate hazards and\\nminimize risks that were identified, or to address work-related incidents that took place, in prior\\nreporting periods.\\nGuidance for Disclosure 403-9-d\\nThis disclosure covers any actions taken or underway to eliminate other work-related hazards\\nand minimize risks (i.e., not covered in Disclosure 403-9-c) using the hierarchy of controls. This\\ndisclosure can include actions taken in response to non- high-consequence  work-related\\ninjuries , and work-related  incidents  with low probability of causing high-consequence injuries.\\nGuidance for Disclosure 403-9-f\\nTypes of worker can be based on criteria such as full-time , part-time , non-guaranteed  hours ,\\npermanent  or temporary  basis, type or degree of control (e.g., control of work or workplace, sole\\nor shared control), and location, among others.\\nGuidance for Disclosure 403-9-g\\nIf the organization follows the ILO code of practice on Recording and notification of occupational\\naccidents and diseases , it can state this in response to Disclosure 403-9-g.\\nIf the organization does not follow the ILO code of practice, it can indicate which system of rules\\nit applies in recording and reporting work-related injuries and its relationship to the ILO code of\\npractice.with complications), to be reported using Disclosures 403-9-a-ii and 403-9-b-ii.\\nGRI 403: Occupational Health and Safety 2018\\n676', metadata={'source': '../input/input_gri.pdf', 'page': 675}),\n",
       "  Document(page_content=\"Note: In the GRI Standards, in some cases, it is specified whether a particular subset of\\nworkers is required to be used. \\nwork-related hazard\\nsource or situation with the potential to cause injury  or ill health\\nSource: International Labour Organization (ILO) Guidelines on Occupational Safety and\\nHealth Management Systems, 2001; modified\\nInternational Organization for Standardization. ISO 45001:2018. Occupational health\\nand safety management systems — Requirements with guidance for use. Geneva:\\nISO, 2018; modified\\nDefinitions that are based on or come from the ISO 14046:2014 and ISO 45001:2018 standards\\nare reproduced with the permission of the International Organization for Standardization, ISO.\\nCopyright remains with ISO.\\nNote: Hazards can be:\\nwork-related incident\\noccurrence arising out of or in the course of work that could or does result in injury  or ill health\\nSource: International Organization for Standardization. ISO 45001:2018. Occupational health\\nand safety management systems — Requirements with guidance for use. Geneva:\\nISO, 2018; modified\\nDefinitions that are based on or come from the ISO 14046:2014 and ISO 45001:2018 standards\\nare reproduced with the permission of the International Organization for Standardization, ISO.\\nCopyright remains with ISO.\\nNote 1: Incidents might be due to, for example, electrical problems, explosion, fire; overflow,\\noverturning, leakage, flow; breakage, bursting, splitting; loss of control, slipping,\\nstumbling and falling; body movement without stress; body movement under/with\\nstress; shock, fright; workplace violence or harassment (e.g., sexual harassment).\\nNote 2: An incident that results in injury or ill health is often referred to as an ‘accident’. An\\nincident that has the potential to result in injury or ill health but where none occurs is\\noften referred to as a ‘ close  call', ‘near-miss’, or ‘near-hit’.\\nwork-related injury or ill health\\nnegative impacts on health arising from exposure  to hazards  at work\\nSource: International Labour Organization (ILO), Guidelines on Occupational Safety and\\nHealth Management Systems, ILO-OSH 2001 , 2001; modified\\nNote 1: ‘Ill health’ indicates damage to health and includes diseases, illnesses, and\\ndisorders. The terms ‘disease’, ‘illness’, and ‘disorder’ are often used\\ninterchangeably and refer to conditions with specific symptoms and diagnoses. physical (e.g., radiation, temperature extremes, constant loud noise, spills on\\nfloors or tripping hazards, unguarded machinery, faulty electrical equipment);•\\nergonomic (e.g., improperly adjusted workstations and chairs, awkward\\nmovements, vibration);•\\nchemical (e.g., exposure to solvents, carbon monoxide, flammable materials, or\\npesticides);•\\nbiological (e.g., exposure to blood and bodily fluids, fungi, bacteria, viruses, or\\ninsect bites);•\\npsychosocial (e.g., verbal abuse, harassment, bullying);•\\nrelated to work-organization (e.g., excessive workload demands, shift work, long\\nhours, night work, workplace violence).•\\nGRI 12: Coal Sector 2022\\n286\", metadata={'source': '../input/input_gri.pdf', 'page': 285}),\n",
       "  Document(page_content='Note 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nBibliography\\nGRI 12: Coal Sector 2022\\n287', metadata={'source': '../input/input_gri.pdf', 'page': 286})]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16cf825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please define high-potential work-related incident',\n",
       "  ' A high-potential work-related incident is an occurrence arising out of or in the course of work that has the potential to result in injury or ill health. Examples of high-potential incidents include electrical problems, explosions, fires, overflows, overturning, leakage, flow, breakage, bursting, splitting, loss of control, slipping, stumbling and falling, body movement without stress, body movement under/with stress, shock, fright, and workplace violence or harassment.')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append((result['question'],result['answer']))\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c8208be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'can you identify other work-related risks',\n",
       " 'chat_history': [('please define high-potential work-related incident',\n",
       "   ' A high-potential work-related incident is an occurrence arising out of or in the course of work that has the potential to result in injury or ill health. Examples of high-potential incidents include electrical problems, explosions, fires, overflows, overturning, leakage, flow, breakage, bursting, splitting, loss of control, slipping, stumbling and falling, body movement without stress, body movement under/with stress, shock, fright, and workplace violence or harassment.')],\n",
       " 'answer': ' Other work-related risks include handling dangerous machinery, tools, vessels, and vehicles; exposure to excessive noise and vibration, causing hearing and other sensory problems; slips, trips, falls from heights, falls overboard, and drowning; working with animals considerably heavier than the worker, lifting heavy weights, and other work giving rise to musculoskeletal disorders; working near people or animals, increasing the risk of exposure to infectious diseases; attacks by wild animals; exposure to dust and potentially harmful organic substances and chemicals; and exposure to extreme temperatures and severe weather.',\n",
       " 'source_documents': [Document(page_content='Note 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.\\nBibliography\\nGRI 403: Occupational Health and Safety 2018\\n686', metadata={'source': '../input/input_gri.pdf', 'page': 685}),\n",
       "  Document(page_content='Note 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nBibliography\\nGRI 12: Coal Sector 2022\\n287', metadata={'source': '../input/input_gri.pdf', 'page': 286}),\n",
       "  Document(page_content='work-related injury or ill health\\nnegative impacts on health arising from exposure  to hazards  at work\\nSource: International Labour Organization (ILO), Guidelines on Occupational Safety and\\nHealth Management Systems, ILO-OSH 2001 , 2001; modified\\nNote 1: ‘Ill health’ indicates damage to health and includes diseases, illnesses, and\\ndisorders. The terms ‘disease’, ‘illness’, and ‘disorder’ are often used\\ninterchangeably and refer to conditions with specific symptoms and diagnoses. \\nNote 2: Work-related injuries and ill health are those that arise from exposure to hazards at\\nwork. Other types of incident can occur that are not connected with the work itself.\\nFor example, the following incidents are not considered to be work related:\\nNote 3: Traveling for work: Injuries and ill health that occur while a worker is traveling are\\nwork related if, at the time of the injury or ill health, the worker was engaged in work\\nactivities ‘in the interest of the employer’. Examples of such activities include\\ntraveling to and from customer contacts; conducting job tasks; and entertaining or\\nbeing entertained to transact, discuss, or promote business (at the direction of the\\nemployer).\\nWorking at home: Injuries and ill health that occur when working at home are work\\nrelated if the injury or ill health occurs while the worker is performing work at home,\\nand the injury or ill health is directly related to the performance of work rather than\\nthe general home environment or setting.\\nMental illness: A mental illness is considered to be work related if it has been\\nnotified voluntarily by the worker and is supported by an opinion from a licensed\\nhealthcare professional with appropriate training and experience stating that the\\nillness is work related.\\nFor more guidance on determining ‘work-relatedness’, see the United States\\nOccupational Safety and Health Administration, Determination of work-relatedness\\n1904.5 , https://www.osha.gov/pls/  oshaweb/owadisp.show_document?\\np_table=STANDARDS&p_id=9636 , accessed on 1 June 2018.\\nNote 4: The terms ‘occupational’ and ‘work-related’ are often used interchangeably.a worker suffers a heart attack while at work that is unconnected with work;•\\na worker driving to or from work is injured in a car accident (where driving is not\\npart of the work, and where the transport has not been organized by the\\nemployer);•\\na worker with epilepsy has a seizure at work that is unconnected with work.•\\nGRI Standards Glossary 2022\\n869', metadata={'source': '../input/input_gri.pdf', 'page': 868}),\n",
       "  Document(page_content=\"Topic 13.19  Occupational health and safety\\nHealthy and safe work conditions are recognized as a human right. Occupational health and safety involves the\\nprevention of physical and mental harm to workers and promotion of workers’ health. This topic covers impacts\\nrelated to workers’ health and safety.\\nAgriculture, aquaculture, and fishing are listed among the most hazardous sectors, with high numbers of work-related\\ninjuries and ill health each year (see references [ 304] and [ 309]). Work-related  hazards  associated with agriculture,\\naquaculture, and fishing include:\\nBecause workers in agriculture, aquaculture, and fishing sectors often live where they work, occupational health and\\nsafety impacts  can also be associated with workers’ living conditions. Adequate working and living conditions provide\\naccess to potable drinking water, quantity and quality of food, hygiene, sanitation, and appropriate accommodation.\\nWorkers are entitled to safe, hygienic, and socially acceptable access to sanitation, a lack thereof can increase the\\nrisk of contracting infectious diseases.\\nWorkers may work long hours and many consecutive days in the agriculture sector, especially when harvesting crops.\\nThey can be exposed to pesticides and other chemical substances used. Children living with workers on farms and\\nplantations can also be exposed to hazardous substances (see also topic  13.6 Pesticides  use and topic  13.17  Child\\nlabor ).\\nFishing is associated with many risks, such as ill health, work-related injuries, and death. Fishing far offshore is\\nconsidered one of the most dangerous occupations. Vessel disasters and falls overboard pose the greatest safety\\nrisks and are the sector’s leading causes of fatalities. Vessel safety risks are linked to weather, lack of weather\\nwarning systems, power loss, engine failure, or inadequate maintenance levels. At-sea crew transfers between\\nfishing vessels and support vessels can pose additional safety risks, especially in rough seas.\\nMost fishing vessels fall outside of size parameters regulated by international maritime safety standards. Small-scale\\nfishers operate millions of fishing vessels that vary in degree of sophistication. Frequently, these vessels prove\\nunsuitable for the conditions in which they may be used, such as carrying considerable amounts of fish or sailing far\\noffshore.\\nVessel safety standards address risks related to general safety, such as fire safety, lighting, ventilation, personal\\nsafety, vessel stability, and survival at sea. Vessel safety training serves to prevent vessel disasters and ensure\\ncompliance with the safety standards. Insurance schemes can further provide income security for fishers and, in case\\nof death or injury, to their families.\\nPrimary fish processing, such as catching, sorting, and storing fish, often requires handling dangerous tools, such as\\nknives and hooks. When fish are manually beheaded, gutted, skinned, or filleted, it is common for workers to\\nexperience cuts or severe lacerations. Fish and other aquatic animals' bites, stings, and tail kicks can also lead to\\ninjuries. In the case of ill health or injury offshore, professional medical care or even an urgent medical evacuation\\nmight be unavailable.\\nFishing can involve long hours at sea, far offshore. The daily and weekly rest requirements determined by crewing\\nlevels can also affect fishing crews' health and safety. Because workers can reside aboard fishing vessels for long\\nperiods, poor living conditions can also disrupt their rest periods. Fishers may also experience difficulty taking shorehandling dangerous machinery, tools, vessels, and vehicles;•\\nexposure to excessive noise and vibration, causing hearing and other sensory problems;•\\nslips, trips, falls from heights, falls overboard, and drowning;•\\nworking with animals considerably heavier than the worker, lifting heavy weights, and other work giving rise to\\nmusculoskeletal disorders;•\\nworking near people or animals, increasing the risk of exposure  to infectious diseases; •\\nattacks by wild animals;•\\nexposure to dust and potentially harmful organic substances and chemicals;•\\nexposure to extreme temperatures and severe weather.•\\nleave or getting off their vessels at foreign ports.\\n \\n \\n \\nGRI 13: Agriculture Aquaculture and Fishing Sectors 2022\\n351\", metadata={'source': '../input/input_gri.pdf', 'page': 350})]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_new=\"can you identify other work-related risks\"\n",
    "output_new=qa({'question':query_new,'chat_history':chat_history})\n",
    "output_new"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Cherif Benham"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
